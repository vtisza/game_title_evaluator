{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analyses on game reviews using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lately I discovered the youtube channel of [*Siraj Raval*](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A) that I can highly recommend for everyone who wants to learn about artificial intelligence meanwhile being highly entertained. In his videos he regularly creates challanges for his audiance to solve. In his latest video that was about making sentiment analysis on an IMDB movie reviews dataset he challanged us to do something simmilar on game reviews. The dataset is open and avaialble at [Kaggle](https://www.kaggle.com/egrinstein/20-years-of-games/).\n",
    "\n",
    "I found it an interesting challange where I can also apply some of my new skills that I just learned on [Practical Deep Learning For Coders MOOC](http://course.fast.ai/) that I also highly recommend for everyone interested in deep learning.\n",
    "\n",
    "Let's see my results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with importing all the libraries I will use. Most importantly:\n",
    "    1. Pandas: reading, storing and manipulating my data\n",
    "    2. Keras: do deep learning (I will use Theano backend but I guess it works similarly with Tensorflow)\n",
    "    3. numpy: matrix operations\n",
    "    4. pickle: saving results (not the model, keras has it's own functions for that)\n",
    "\n",
    "If you want to reproduce my notebook please note that I use Python 3.5 (i.e. urllib works little differently in Python 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import urllib\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already downloaded the database (that is a single csv file). I just read it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_phrase</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>platform</th>\n",
       "      <th>score</th>\n",
       "      <th>genre</th>\n",
       "      <th>editors_choice</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>LittleBigPlanet PS Vita</td>\n",
       "      <td>/games/littlebigplanet-vita/vita-98907</td>\n",
       "      <td>PlayStation Vita</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Platformer</td>\n",
       "      <td>Y</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>LittleBigPlanet PS Vita -- Marvel Super Hero E...</td>\n",
       "      <td>/games/littlebigplanet-ps-vita-marvel-super-he...</td>\n",
       "      <td>PlayStation Vita</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Platformer</td>\n",
       "      <td>Y</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great</td>\n",
       "      <td>Splice: Tree of Life</td>\n",
       "      <td>/games/splice/ipad-141070</td>\n",
       "      <td>iPad</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Puzzle</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great</td>\n",
       "      <td>NHL 13</td>\n",
       "      <td>/games/nhl-13/xbox-360-128182</td>\n",
       "      <td>Xbox 360</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Sports</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great</td>\n",
       "      <td>NHL 13</td>\n",
       "      <td>/games/nhl-13/ps3-128181</td>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Sports</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  score_phrase                                              title  \\\n",
       "0      Amazing                            LittleBigPlanet PS Vita   \n",
       "1      Amazing  LittleBigPlanet PS Vita -- Marvel Super Hero E...   \n",
       "2        Great                               Splice: Tree of Life   \n",
       "3        Great                                             NHL 13   \n",
       "4        Great                                             NHL 13   \n",
       "\n",
       "                                                 url          platform  score  \\\n",
       "0             /games/littlebigplanet-vita/vita-98907  PlayStation Vita    9.0   \n",
       "1  /games/littlebigplanet-ps-vita-marvel-super-he...  PlayStation Vita    9.0   \n",
       "2                          /games/splice/ipad-141070              iPad    8.5   \n",
       "3                      /games/nhl-13/xbox-360-128182          Xbox 360    8.5   \n",
       "4                           /games/nhl-13/ps3-128181     PlayStation 3    8.5   \n",
       "\n",
       "        genre editors_choice  release_year  release_month  release_day  \n",
       "0  Platformer              Y          2012              9           12  \n",
       "1  Platformer              Y          2012              9           12  \n",
       "2      Puzzle              N          2012              9           12  \n",
       "3      Sports              N          2012              9           11  \n",
       "4      Sports              N          2012              9           11  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=pd.read_csv(\"ign.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to predict the \t*score_phrase* of the reviews but we also have numerical scores. Let's check the distribution of score_phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_phrase\n",
       "Amazing        1804\n",
       "Awful           664\n",
       "Bad            1269\n",
       "Disaster          3\n",
       "Good           4741\n",
       "Great          4773\n",
       "Masterpiece      55\n",
       "Mediocre       1959\n",
       "Okay           2945\n",
       "Painful         340\n",
       "Unbearable       72\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.groupby(\"score_phrase\").count().title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the categories are not really balanced and some category contains just really few elements (i.e. Disaster only 3). The phrases probably follow some ordinality so may we can use just the score to infer the phrases. We can check if our assumption is right by checking the mean scoe belonging to each phrases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_phrase\n",
       "Disaster        0.666667\n",
       "Unbearable      1.290278\n",
       "Painful         2.267941\n",
       "Awful           3.290211\n",
       "Bad             4.331600\n",
       "Mediocre        5.318530\n",
       "Okay            6.366553\n",
       "Good            7.369099\n",
       "Great           8.316510\n",
       "Amazing         9.176663\n",
       "Masterpiece    10.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.groupby('score_phrase')['score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order is quite visible. So my plan is to predict the score of a review and connect it to the category with the closest value. I create a simple function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_scores=reviews.groupby('score_phrase')['score'].mean().sort_values()\n",
    "score_list=mean_scores.tolist()\n",
    "phrase_score_dict=mean_scores.to_dict()\n",
    "score_phrase_dict= {v:k for k,v in phrase_score_dict.items()}\n",
    "\n",
    "def phrase_to_score(score,score_list=score_list):\n",
    "    return score_phrase_dict[min(score_list, key=lambda x:abs(x-score))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I clean and tokenize the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z0-9]')\n",
    "reviews[\"title_2\"]=reviews[\"title\"].apply(lambda x: regex.sub(' ', x).lower().strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of the number of words in the titles and the total number of different words used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x972693cc18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9726497a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews.head()\n",
    "reviews[\"title_2\"].apply(len).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different number of words used: 7938 \n"
     ]
    }
   ],
   "source": [
    "words=set(list(chain.from_iterable(reviews[\"title_2\"].tolist())))\n",
    "number_of_words=len(words)\n",
    "print(\"Different number of words used: {} \".format(number_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making dictionaries to translate beetween words and word ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_frequency_dict=Counter(list(chain.from_iterable(reviews[\"title_2\"].tolist())))\n",
    "word_idx_dict={word: idx for idx, word in enumerate(sorted(word_frequency_dict, key=lambda x: -word_frequency_dict[x]))}\n",
    "idx_word_dict={idx: word for idx, word in enumerate(sorted(word_frequency_dict, key=lambda x: -word_frequency_dict[x]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_phrase</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>platform</th>\n",
       "      <th>score</th>\n",
       "      <th>genre</th>\n",
       "      <th>editors_choice</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_day</th>\n",
       "      <th>title_2</th>\n",
       "      <th>title_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>LittleBigPlanet PS Vita</td>\n",
       "      <td>/games/littlebigplanet-vita/vita-98907</td>\n",
       "      <td>PlayStation Vita</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Platformer</td>\n",
       "      <td>Y</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>[littlebigplanet, ps, vita]</td>\n",
       "      <td>[1324, 4043, 2858]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>LittleBigPlanet PS Vita -- Marvel Super Hero E...</td>\n",
       "      <td>/games/littlebigplanet-ps-vita-marvel-super-he...</td>\n",
       "      <td>PlayStation Vita</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Platformer</td>\n",
       "      <td>Y</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>[littlebigplanet, ps, vita, marvel, super, her...</td>\n",
       "      <td>[1324, 4043, 2858, 193, 17, 51, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great</td>\n",
       "      <td>Splice: Tree of Life</td>\n",
       "      <td>/games/splice/ipad-141070</td>\n",
       "      <td>iPad</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Puzzle</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>[splice, tree, of, life]</td>\n",
       "      <td>[5982, 1926, 1, 146]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great</td>\n",
       "      <td>NHL 13</td>\n",
       "      <td>/games/nhl-13/xbox-360-128182</td>\n",
       "      <td>Xbox 360</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Sports</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>[nhl, 13]</td>\n",
       "      <td>[53, 592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great</td>\n",
       "      <td>NHL 13</td>\n",
       "      <td>/games/nhl-13/ps3-128181</td>\n",
       "      <td>PlayStation 3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Sports</td>\n",
       "      <td>N</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>[nhl, 13]</td>\n",
       "      <td>[53, 592]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  score_phrase                                              title  \\\n",
       "0      Amazing                            LittleBigPlanet PS Vita   \n",
       "1      Amazing  LittleBigPlanet PS Vita -- Marvel Super Hero E...   \n",
       "2        Great                               Splice: Tree of Life   \n",
       "3        Great                                             NHL 13   \n",
       "4        Great                                             NHL 13   \n",
       "\n",
       "                                                 url          platform  score  \\\n",
       "0             /games/littlebigplanet-vita/vita-98907  PlayStation Vita    9.0   \n",
       "1  /games/littlebigplanet-ps-vita-marvel-super-he...  PlayStation Vita    9.0   \n",
       "2                          /games/splice/ipad-141070              iPad    8.5   \n",
       "3                      /games/nhl-13/xbox-360-128182          Xbox 360    8.5   \n",
       "4                           /games/nhl-13/ps3-128181     PlayStation 3    8.5   \n",
       "\n",
       "        genre editors_choice  release_year  release_month  release_day  \\\n",
       "0  Platformer              Y          2012              9           12   \n",
       "1  Platformer              Y          2012              9           12   \n",
       "2      Puzzle              N          2012              9           12   \n",
       "3      Sports              N          2012              9           11   \n",
       "4      Sports              N          2012              9           11   \n",
       "\n",
       "                                             title_2  \\\n",
       "0                        [littlebigplanet, ps, vita]   \n",
       "1  [littlebigplanet, ps, vita, marvel, super, her...   \n",
       "2                           [splice, tree, of, life]   \n",
       "3                                          [nhl, 13]   \n",
       "4                                          [nhl, 13]   \n",
       "\n",
       "                              title_3  \n",
       "0                  [1324, 4043, 2858]  \n",
       "1  [1324, 4043, 2858, 193, 17, 51, 6]  \n",
       "2                [5982, 1926, 1, 146]  \n",
       "3                           [53, 592]  \n",
       "4                           [53, 592]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"title_3\"]=reviews[\"title_2\"].apply(lambda x: [word_idx_dict[y] for y in x])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide our dataset randomly to a train (80%) and a validation (20%) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx=np.random.uniform(size=len(reviews))\n",
    "train_x=reviews[train_idx<0.8][\"title_3\"]\n",
    "train_y=reviews[train_idx<0.8][\"score\"]\n",
    "\n",
    "valid_x=reviews[train_idx>=0.8][\"title_3\"]\n",
    "valid_y=reviews[train_idx>=0.8][\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the 5000 most frequent word for later data embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "length=10\n",
    "\n",
    "train_x = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in train_x]\n",
    "valid_x = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in valid_x]\n",
    "train_x=pad_sequences(train_x, maxlen=length, value=0)\n",
    "valid_x=pad_sequences(valid_x, maxlen=length, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building up the model architecture. No rule for how to do it. I played around a little with different architectures and this one proved to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=length),\n",
    "    BatchNormalization(),\n",
    "    SimpleRNN(100, consume_less='gpu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='relu')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 10, 50)        250000      embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 10, 50)        200         embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_3 (SimpleRNN)          (None, 100)           15100       batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 100)           10100       simplernn_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 100)           400         dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 100)           0           batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 20)            2020        dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 20)            80          dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 20)            0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             21          dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 277,921\n",
      "Trainable params: 277,581\n",
      "Non-trainable params: 340\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are ready with model architecture we can start to train our model. Originaly I used high nuber of epochs with early stopping (when model was not improving anymore) and ModelCheckpoint to save the best result. Unfortunately they proved to be little bugy so I switched them off and started to use lower number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14773 samples, validate on 3852 samples\n",
      "Epoch 1/20\n",
      "14773/14773 [==============================] - 2s - loss: 2.5713 - mean_squared_error: 2.5713 - val_loss: 3.1887 - val_mean_squared_error: 3.1887\n",
      "Epoch 2/20\n",
      "14773/14773 [==============================] - 2s - loss: 2.6440 - mean_squared_error: 2.6440 - val_loss: 11.8149 - val_mean_squared_error: 11.8149\n",
      "Epoch 3/20\n",
      "14773/14773 [==============================] - 2s - loss: 2.4930 - mean_squared_error: 2.4930 - val_loss: 5.1213 - val_mean_squared_error: 5.1213\n",
      "Epoch 4/20\n",
      "14773/14773 [==============================] - 2s - loss: 2.1842 - mean_squared_error: 2.1842 - val_loss: 48.5494 - val_mean_squared_error: 48.5494\n",
      "Epoch 5/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.9347 - mean_squared_error: 1.9347 - val_loss: 2.6707 - val_mean_squared_error: 2.6707\n",
      "Epoch 6/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.8702 - mean_squared_error: 1.8702 - val_loss: 2.8271 - val_mean_squared_error: 2.8271\n",
      "Epoch 7/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.7833 - mean_squared_error: 1.7833 - val_loss: 2.3491 - val_mean_squared_error: 2.3491\n",
      "Epoch 8/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.7648 - mean_squared_error: 1.7648 - val_loss: 2.5816 - val_mean_squared_error: 2.5816\n",
      "Epoch 9/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 2.2880 - val_mean_squared_error: 2.2880\n",
      "Epoch 10/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.6362 - mean_squared_error: 1.6362 - val_loss: 2.3168 - val_mean_squared_error: 2.3168\n",
      "Epoch 11/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.5827 - mean_squared_error: 1.5827 - val_loss: 2.3762 - val_mean_squared_error: 2.3762\n",
      "Epoch 12/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.6026 - mean_squared_error: 1.6026 - val_loss: 2.3775 - val_mean_squared_error: 2.3775\n",
      "Epoch 13/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.5440 - mean_squared_error: 1.5440 - val_loss: 2.3488 - val_mean_squared_error: 2.3488\n",
      "Epoch 14/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.5442 - mean_squared_error: 1.5442 - val_loss: 2.2816 - val_mean_squared_error: 2.2816\n",
      "Epoch 15/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.4839 - mean_squared_error: 1.4839 - val_loss: 2.2736 - val_mean_squared_error: 2.2736\n",
      "Epoch 16/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.4761 - mean_squared_error: 1.4761 - val_loss: 2.3163 - val_mean_squared_error: 2.3163\n",
      "Epoch 17/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.4492 - mean_squared_error: 1.4492 - val_loss: 2.3144 - val_mean_squared_error: 2.3144\n",
      "Epoch 18/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.3854 - mean_squared_error: 1.3854 - val_loss: 2.3894 - val_mean_squared_error: 2.3894\n",
      "Epoch 19/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.3682 - mean_squared_error: 1.3682 - val_loss: 2.3145 - val_mean_squared_error: 2.3145\n",
      "Epoch 20/20\n",
      "14773/14773 [==============================] - 2s - loss: 1.3367 - mean_squared_error: 1.3367 - val_loss: 2.2928 - val_mean_squared_error: 2.2928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x9739e688d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=0, save_best_only=True)\n",
    "#model.fit(train_x, train_y, validation_data=(valid_x, valid_y), nb_epoch=100, batch_size=128, callbacks=[early_stopping,checkpointer])\n",
    "model.fit(train_x, train_y, validation_data=(valid_x, valid_y), nb_epoch=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model doesn't seem to iprove further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo labeling is a useful technique to increase the efectivness of our model. We predict the labels for our validation test and use it together with the training set to further train our model. We do not use original validation labels just the predicted ones. (otherwise improving results would only be the effect of everfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_y_pred=model.predict(valid_x)\n",
    "valid_y_pred=list(chain.from_iterable(valid_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pseudo_train_x=np.concatenate((train_x,valid_x), axis=0)\n",
    "pseudo_train_y=np.concatenate((np.array(train_y),np.array(valid_y_pred)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18625 samples, validate on 3852 samples\n",
      "Epoch 1/2\n",
      "18625/18625 [==============================] - 3s - loss: 0.8590 - mean_squared_error: 0.8590 - val_loss: 2.2818 - val_mean_squared_error: 2.2818\n",
      "Epoch 2/2\n",
      "18625/18625 [==============================] - 3s - loss: 0.8561 - mean_squared_error: 0.8561 - val_loss: 2.2658 - val_mean_squared_error: 2.2658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x973b575320>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pseudo_train_x, pseudo_train_y, validation_data=(valid_x, valid_y), nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18625 samples, validate on 3852 samples\n",
      "Epoch 1/2\n",
      "18625/18625 [==============================] - 3s - loss: 0.7511 - mean_squared_error: 0.7511 - val_loss: 2.2440 - val_mean_squared_error: 2.2440\n",
      "Epoch 2/2\n",
      "18625/18625 [==============================] - 3s - loss: 0.7653 - mean_squared_error: 0.7653 - val_loss: 2.2086 - val_mean_squared_error: 2.2086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x973b575780>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pseudo_train_x, pseudo_train_y, validation_data=(valid_x, valid_y), nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though improvment is very small but pseudo labeling was actually able to impove our model. MSE is 2.2086 that means our model makes less than 1.5 points error in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple function to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eligible_words=[k for k,v in word_idx_dict.items() if v<5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def predictor(title_orig=\"No title\", model=model):\n",
    "    regex = re.compile('[^a-zA-Z0-9]')\n",
    "    title=regex.sub(' ', title_orig).lower().strip().split()\n",
    "    title=[word_idx_dict[word] if word in eligible_words else 4999 for word in title]\n",
    "    title=[title]\n",
    "    title=pad_sequences(title, maxlen=10, value=0)\n",
    "    score=model.predict(title)[0][0]\n",
    "    phrase=score_phrase_dict[min(score_list, key=lambda x:abs(x-score))]\n",
    "    print(\"'{}' would receive {:0.1f}. It is {} rating.\".format(title_orig,score,phrase))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to predict an original game title idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'wolfeinstein V: last chapter' would receive 7.4. It is Good rating.\n"
     ]
    }
   ],
   "source": [
    "predictor(\"wolfeinstein V: last chapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save objects for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save certain object necessary to build a predicting app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"bestModel_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_structure=open(\"model/model.json\", \"w\")\n",
    "model_structure.write(model.to_json())\n",
    "model_structure.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( eligible_words, open( \"model/eligible_words.p\", \"wb\" ) )\n",
    "pickle.dump( word_idx_dict, open( \"model/word_idx_dict.p\", \"wb\" ) )\n",
    "pickle.dump( score_phrase_dict, open( \"model/score_phrase_dict.p\", \"wb\" ) )\n",
    "pickle.dump(score_list, open(\"model/score_list\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model/bestModel_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are surprising. Using only the title of computer games we can effectively predict the score of a computer game. We made a model with less than 1.5 points mean absolute error. Next step is to put this model into work in a neat web app.\n",
    "\n",
    "Stay tuned! :smile:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
